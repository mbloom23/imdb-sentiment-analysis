{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# Implement and train a LSTM for sentiment analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqSTICmcTc2i",
        "outputId": "2ee949e9-f18a-4b32-cea2-ba75aaeed3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6ymxu99xZk"
      },
      "source": [
        "## Step 0: set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spc_UH4B9xZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9f30e3-fe63-4015-84d3-e8b01f371879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"resources\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-KFXJ1XTJh9"
      },
      "source": [
        "### Hyperparameters. Do not directly touch this to mess up settings.\n",
        "\n",
        "If you want to initalize new hyperparameter sets, use \"new_hparams = HyperParams()\" and change corresponding fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxnFjs3f9xZn"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.PAD_INDEX = 0\n",
        "        self.UNK_INDEX = 1\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "        self.MAX_LENGTH = 256\n",
        "        self.BATCH_SIZE = 96\n",
        "        self.EMBEDDING_DIM = 1\n",
        "        self.HIDDEN_DIM = 100\n",
        "        self.OUTPUT_DIM = 2\n",
        "        self.N_LAYERS = 1\n",
        "        self.DROPOUT_RATE = 0.0\n",
        "        self.LR = 0.01\n",
        "        self.N_EPOCHS = 5\n",
        "        self.WD = 0\n",
        "        self.OPTIM = \"sgd\"\n",
        "        self.BIDIRECTIONAL = False\n",
        "        self.SEED = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Lab 1(a) Implement your own data loader function.  \n",
        "First, you need to read the data from the dataset file on the local disk. \n",
        "Then, split the dataset into three sets: train, validation and test by 7:1:2 ratio.\n",
        "Finally return x_train, x_valid, x_test, y_train, y_valid, y_test where x represents reviews and y represent labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD7HSvM19xZp"
      },
      "outputs": [],
      "source": [
        "def load_imdb(base_csv:str = '/content/drive/MyDrive/Duke/2022-2023/ECE 661/HW3/HW3 Code/IMDBDataset.csv'):\n",
        "    # Load the IMDB dataset\n",
        "    # param base_csv: the path of the dataset file.\n",
        "    # return: train, validation and test set.\n",
        "    \n",
        "    # Add your code here. \n",
        "    # load the data, separate into reviews (data) and sentiments (labels)\n",
        "    data = pd.read_csv(base_csv)\n",
        "    # extract just the values, pd dataframes include indices\n",
        "    x = data['review'].values\n",
        "    y = data['sentiment'].values\n",
        "    \n",
        "    # split the data into 7:1:2 (35000 validation, 5000 validation, 10000 test)\n",
        "    # first split into 8:2 training and validation, test\n",
        "    x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)\n",
        "    # then split the training and validation into 7:8 training, validation\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train_val, y_train_val, train_size=0.875, test_size=0.125)\n",
        "\n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Lab 1(b): Implement your function to build a vocabulary based on the training corpus.\n",
        "Implement the build_vocab function to build a vocabulary based on the training corpus.\n",
        "You should first compute the frequency of all the words in the training corpus. Remove the words\n",
        "that are in the STOP_WORDS. Then filter the words by their frequency (â‰¥ min_freq) and finally\n",
        "generate a corpus variable that contains a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=5, hparams=None) -> dict:\n",
        "    # build a vocabulary based on the training corpus.\n",
        "    # param x_train:  List. The training corpus. Each sample in the list is a string of text.\n",
        "    # param min_freq: Int. The frequency threshold for selecting words.\n",
        "    # return: dictionary {word:index}\n",
        "    \n",
        "    # Add your code here. Your code should assign corpus with a list of words.\n",
        "    corpus = {} # placeholder dictionary\n",
        "    for review in x_train:\n",
        "      for word in review.lower().split(): # standardize all words to lowercase \n",
        "        # each list item is a review\n",
        "        # for each word in each review, as long as the word isn't in STOP_WORDS, add or increment word and its frequency\n",
        "        if not(word in hparams.STOP_WORDS):\n",
        "          # if the word isn't in the corpus yet, add it and set its frequency to 0\n",
        "          if not(word in corpus.keys()):\n",
        "            corpus[word] = 0\n",
        "          # increment the word's frequency\n",
        "          corpus[word] += 1\n",
        "\n",
        "    # removing words if they occur less than min_freq times\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus_, key=corpus.get, reverse=True) # is this needed???\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Lab 1(c): Implement your tokenize function. \n",
        "For each word, find its index in the vocabulary. \n",
        "Return a list of int that represents the indices of words in the example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    # Tokenize the give example string into a list of token indices.\n",
        "    # param vocab: dict, the vocabulary.\n",
        "    # param example: a string of text.\n",
        "    # return: a list of token indices.\n",
        "    \n",
        "    # Your code here.\n",
        "    tokens = []\n",
        "    # split the example string into words, for each word find the index in the vocab dict\n",
        "    for word in example.lower().split(): # again, standardize all words to lowercase\n",
        "      if word in vocab.keys(): # don't tokenize a word that isn't in the vocab (stopwords)\n",
        "        tokens.append(vocab[word])\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Lab 1 (d): Implement the __getitem__ function. Given an index i, you should return the i-th review and label. \n",
        "The review is originally a string. Please tokenize it into a sequence of token indices. \n",
        "Use the max_length parameter to truncate the sequence so that it contains at most max_length tokens. \n",
        "Convert the label string ('positive'/'negative') to a binary index. 'positive' is 1 and 'negative' is 0. \n",
        "Return a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
        "        # param x: list of reviews\n",
        "        # param y: list of labels\n",
        "        # param vocab: vocabulary dictionary {word:index}.\n",
        "        # param max_length: the maximum sequence length.\n",
        "        \n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Return the tokenized review and label by the given index.\n",
        "        # param idx: index of the sample.\n",
        "        # return: a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label.\n",
        "        \n",
        "        # Add your code here.\n",
        "        # tokenize the string, shorten it to <=max_length\n",
        "        tokens = tokenize(self.vocab, self.x[idx])\n",
        "        tokens = tokens[:self.max_length]\n",
        "\n",
        "        # if the sentiment is positive, label 1, if negative label 0\n",
        "        sentiment = 0\n",
        "        if (self.y[idx] == 'positive'):\n",
        "          sentiment = 1\n",
        "        else:\n",
        "          sentiment = 0\n",
        "        # create a dictionary with the information needed\n",
        "        info = {'ids':tokens,\n",
        "                'length':len(tokens), # should this be the length of the truncated sequence or the original length??\n",
        "                'label':sentiment}\n",
        "        return info \n",
        "        \n",
        "    \n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## Lab 1 (e): Implement the LSTM model for sentiment analysis.\n",
        "Q(a): Implement the initialization function.\n",
        "Your task is to create the model by stacking several necessary layers including an embedding layer, a lstm cell, a linear layer, and a dropout layer.\n",
        "You can call functions from Pytorch's nn library. For example, nn.Embedding, nn.LSTM, nn.Linear.<br>\n",
        "Q(b): Implement the forward function.\n",
        "    Decide where to apply dropout. \n",
        "    The sequences in the batch have different lengths. Write/call a function to pad the sequences into the same length. \n",
        "    Apply a fully-connected (fc) layer to the output of the LSTM layer. \n",
        "    Return the output features which is of size [batch size, output dim]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "                \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \n",
        "        # Create a LSTM model for classification.\n",
        "        # :param vocab_size: size of the vocabulary\n",
        "        # :param embedding_dim: dimension of embeddings\n",
        "        # :param hidden_dim: dimension of hidden features\n",
        "        # :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        # :param n_layers: number of layers.\n",
        "        # :param dropout_rate: dropout rate.\n",
        "        # :param pad_index: index of the padding token.we\n",
        "        \n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_index)\n",
        "        # is a separate nn.Dropout module needed or is the built-in dropout fine?\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout_rate, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim) \n",
        "\n",
        "        # Weight initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        # Feed the given token ids to the model.\n",
        "        # :param ids: [batch size, seq len] batch of token ids.\n",
        "        # :param length: [batch size] batch of length of the token ids.\n",
        "        # :return: prediction of size [batch size, output dim].\n",
        "        \n",
        "        # Add your code here.\n",
        "        prediction = self.embed(ids)\n",
        "        # pack embeddings before passing to LSTM\n",
        "        prediction = nn.utils.rnn.pack_padded_sequence(prediction, length, batch_first=True, enforce_sorted=False)\n",
        "        # extract the final hidden state\n",
        "        prediction = self.lstm(prediction)[1][0][-1]\n",
        "        # flatten and pass to FC layer\n",
        "        prediction = prediction.view(prediction.size(0), -1)\n",
        "        prediction = self.fc(prediction)\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "        scheduler.step()\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy\n",
        "\n",
        "def predict_sentiment(text, model, vocab, device):\n",
        "    tokens = tokenize(vocab, text)\n",
        "    ids = [vocab[t] if t in vocab else UNK_INDEX for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocK7AMIoTJiB"
      },
      "source": [
        "### Lab 1 (g) Implement GRU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-IETNgcTJiB"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \n",
        "        # Create a LSTM model for classification.\n",
        "        # :param vocab_size: size of the vocabulary\n",
        "        # :param embedding_dim: dimension of embeddings\n",
        "        # :param hidden_dim: dimension of hidden features\n",
        "        # :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        # :param n_layers: number of layers.\n",
        "        # :param dropout_rate: dropout rate.\n",
        "        # :param pad_index: index of the padding token.we\n",
        "        \n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_index)\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout_rate, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "        # Weight Initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        # Feed the given token ids to the model.\n",
        "        # :param ids: [batch size, seq len] batch of token ids.\n",
        "        # :param length: [batch size] batch of length of the token ids.\n",
        "        # :return: prediction of size [batch size, output dim].\n",
        "        \n",
        "        # Add your code here.\n",
        "        prediction = self.embed(ids)\n",
        "        prediction = nn.utils.rnn.pack_padded_sequence(prediction, length, batch_first=True, enforce_sorted=False)\n",
        "        prediction = self.gru(prediction)[1][-1]\n",
        "        prediction = torch.flatten(prediction, 1)\n",
        "        prediction = self.fc(prediction)\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkkTnT7TJiB"
      },
      "source": [
        "### Learning rate warmup. DO NOT TOUCH!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEZHRTqGTJiB"
      },
      "outputs": [],
      "source": [
        "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        num_warmup_steps: int,\n",
        "    ):\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_warmup_steps:\n",
        "            # warmup\n",
        "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
        "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
        "            self.last_lr = lr\n",
        "        else:\n",
        "            lr = self.base_lrs\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K7XzCUCTJiB"
      },
      "source": [
        "### Implement the training / validation iteration here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
        "    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!\n",
        "    # Set the random seeds.\n",
        "    torch.manual_seed(hparams.SEED)\n",
        "    random.seed(hparams.SEED)\n",
        "    np.random.seed(hparams.SEED)\n",
        "\n",
        "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()\n",
        "    vocab = build_vocab(x_train, hparams=hparams)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f'Length of vocabulary is {vocab_size}')\n",
        "\n",
        "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
        "    valid_data = IMDB(x_valid, y_valid, vocab, hparams.MAX_LENGTH)\n",
        "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
        "\n",
        "    collate = functools.partial(collate_fn, pad_index=hparams.PAD_INDEX)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    \n",
        "    # Model\n",
        "    if \"override_models_with_gru\" in kwargs and kwargs[\"override_models_with_gru\"]:\n",
        "        model = GRU(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    else:\n",
        "        model = LSTM(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimization. Lab 2 (a)(b) should choose one of them.\n",
        "    # DO NOT TOUCH optimizer-specific hyperparameters! (e.g., eps, momentum)\n",
        "    # DO NOT change optimizer implementations!\n",
        "    if hparams.OPTIM == \"sgd\":\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)        \n",
        "    elif hparams.OPTIM == \"adagrad\":\n",
        "        optimizer = optim.Adagrad(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"adam\":\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"rmsprop\":\n",
        "        optimizer = optim.RMSprop(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Optimizer not implemented!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Start training\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    \n",
        "    # Warmup Scheduler. DO NOT TOUCH!\n",
        "    WARMUP_STEPS = 200\n",
        "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
        "\n",
        "    for epoch in range(hparams.N_EPOCHS):\n",
        "        \n",
        "        # Your code: implement the training process and save the best model.\n",
        "        \n",
        "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "        \n",
        "        epoch_train_loss = np.mean(train_loss)\n",
        "        epoch_train_acc = np.mean(train_acc)\n",
        "        epoch_valid_loss = np.mean(valid_loss)\n",
        "        epoch_valid_acc = np.mean(valid_acc)\n",
        "\n",
        "        # Save the model that achieves the smallest validation loss.\n",
        "        if epoch_valid_loss < best_valid_loss:\n",
        "            # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
        "            best_valid_loss = epoch_valid_loss\n",
        "            if not os.path.exists('./saved_model'):\n",
        "              os.makedirs('./saved_model')\n",
        "            print('Saving... ')\n",
        "            state = {'state_dict':model.state_dict(),\n",
        "                     'epoch':epoch,\n",
        "                     'lr':lr_scheduler.get_lr()}\n",
        "            torch.save(state, os.path.join('./saved_model', 'RNN.pth'))\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "\n",
        "    # Your Code: Load the best model's weights.\n",
        "    if \"override_models_with_gru\" in kwargs and kwargs[\"override_models_with_gru\"]:\n",
        "      model = GRU(vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    else:\n",
        "      model = LSTM(vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    state_dict = torch.load('./saved_model/RNN.pth')\n",
        "    model.load_state_dict(state_dict['state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
        "    test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "\n",
        "    epoch_test_loss = np.mean(test_loss)\n",
        "    epoch_test_acc = np.mean(test_acc)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "    \n",
        "    # Free memory for later usage.\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return {\n",
        "        'num_params': num_params,\n",
        "        \"test_loss\": epoch_test_loss,\n",
        "        \"test_acc\": epoch_test_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKCu4rPBA2Sp"
      },
      "source": [
        "### Lab 1 (f): Train model with original hyperparameters, for LSTM.\n",
        "\n",
        "Train the model with default hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIR3VH0KTJiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e778853-2556-4543-8660-1e57deb5b6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.93it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.497\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.65it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.501\n",
            "valid_loss: 0.693, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.67it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.89it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.693, train_acc: 0.502\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.81it/s]\n",
            "Saving... \n",
            "epoch: 4\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.80it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.47it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.693, train_acc: 0.501\n",
            "valid_loss: 0.694, valid_acc: 0.498\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 86.50it/s]\n",
            "test_loss: 0.693, test_acc: 0.502\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiEgtP-oTJiE"
      },
      "source": [
        "### Lab 1 (h) Train GRU with vanilla hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_llD1PuTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e0c145-c421-4273-dc48-673275735e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 87,560 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.01it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.694, train_acc: 0.498\n",
            "valid_loss: 0.694, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 82.23it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.694, train_acc: 0.495\n",
            "valid_loss: 0.693, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.50it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.694, train_acc: 0.502\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.21it/s]\n",
            "Saving... \n",
            "epoch: 4\n",
            "train_loss: 0.694, train_acc: 0.496\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.92it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.39it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.694, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.498\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 87.05it/s]\n",
            "test_loss: 0.693, test_acc: 0.502\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = [0.694,0.694,0.694,0.694,0.694]\n",
        "valid_losses = [0.694,0.693,0.693,0.693,0.693]\n",
        "plt.plot(np.arange(1,6), train_losses, label='Training Loss')\n",
        "plt.plot(np.arange(1,6), valid_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.ylim(0.6,.8)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title(\"GRU Training Curve with Vanilla Hyperparameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "KTScEWbn4p3j",
        "outputId": "bd967854-ddae-4e88-9a4e-d39cc868a3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'GRU Training Curve with Vanilla Hyperparameters')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUB7Bw0oTJiE"
      },
      "source": [
        "### Lab 2 (a) Study of LSTM Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ5xdP2FTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2454aee8-f68a-45a8-d9f4-594c1d24406b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.20it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.79it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.504\n",
            "valid_loss: 0.693, valid_acc: 0.507\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.06it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.644, train_acc: 0.670\n",
            "valid_loss: 0.570, valid_acc: 0.795\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.05it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.47it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.538, train_acc: 0.813\n",
            "valid_loss: 0.538, valid_acc: 0.817\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.45it/s]\n",
            "Saving... \n",
            "epoch: 4\n",
            "train_loss: 0.495, train_acc: 0.839\n",
            "valid_loss: 0.524, valid_acc: 0.827\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.10it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.35it/s]\n",
            "Saving... \n",
            "epoch: 5\n",
            "train_loss: 0.463, train_acc: 0.853\n",
            "valid_loss: 0.513, valid_acc: 0.832\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 85.81it/s]\n",
            "test_loss: 0.501, test_acc: 0.832\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adagrad'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, \"lstm_1layer_adagrad\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adagrad'\n",
        "_ = train_and_test_model_with_hparams(new_hparams, \"lstm_1layer_adagrad_lr_01\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7w-FEhysP6a",
        "outputId": "a9181fdb-93ea-44c9-cddd-f3b5dedd0300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.08it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.51it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.682, train_acc: 0.602\n",
            "valid_loss: 0.377, valid_acc: 0.855\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.60it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.343, train_acc: 0.868\n",
            "valid_loss: 0.304, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.52it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.84it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.223, train_acc: 0.919\n",
            "valid_loss: 0.285, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.30it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 55.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.162, train_acc: 0.945\n",
            "valid_loss: 0.291, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.67it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.125, train_acc: 0.960\n",
            "valid_loss: 0.318, valid_acc: 0.883\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 86.17it/s]\n",
            "test_loss: 0.287, test_acc: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_rmsprop')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqOmbaurrL82",
        "outputId": "3b27c7d6-950e-4d5a-9784-9f94d019ecd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.40it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.608, train_acc: 0.683\n",
            "valid_loss: 0.613, valid_acc: 0.637\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.76it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.297, train_acc: 0.873\n",
            "valid_loss: 0.301, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.43it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.149, train_acc: 0.947\n",
            "valid_loss: 0.310, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.56it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.080, train_acc: 0.974\n",
            "valid_loss: 0.375, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.40it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.049, train_acc: 0.984\n",
            "valid_loss: 0.579, valid_acc: 0.863\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 85.20it/s]\n",
            "test_loss: 0.296, test_acc: 0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_rmsprop_lr_01')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulyxwi7CtRD4",
        "outputId": "b0167efa-307c-47e2-d367-b2d521d6c3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.72it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.63it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.536, train_acc: 0.730\n",
            "valid_loss: 0.379, valid_acc: 0.831\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.35it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.334, train_acc: 0.864\n",
            "valid_loss: 0.377, valid_acc: 0.847\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.55it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.55it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.246, train_acc: 0.905\n",
            "valid_loss: 0.350, valid_acc: 0.860\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.60it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.182, train_acc: 0.932\n",
            "valid_loss: 0.365, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.14it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.144, train_acc: 0.950\n",
            "valid_loss: 0.452, valid_acc: 0.854\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.28it/s]\n",
            "test_loss: 0.356, test_acc: 0.857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocAj5z7ku0lb",
        "outputId": "368567b0-ba71-4bcf-d550-673d251af8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.41it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.682, train_acc: 0.575\n",
            "valid_loss: 0.603, valid_acc: 0.747\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.56it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.74it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.307, train_acc: 0.878\n",
            "valid_loss: 0.283, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.31it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.152, train_acc: 0.947\n",
            "valid_loss: 0.320, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.66it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.091, train_acc: 0.971\n",
            "valid_loss: 0.348, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.44it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.044, train_acc: 0.987\n",
            "valid_loss: 0.442, valid_acc: 0.852\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 90.35it/s]\n",
            "test_loss: 0.282, test_acc: 0.883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_lr_01')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ7lyfAlybdy",
        "outputId": "1d36adca-60fc-49e2-da59-cccbcdcb71a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 97,860 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.02it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.629, train_acc: 0.642\n",
            "valid_loss: 0.422, valid_acc: 0.818\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.13it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.18it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.288, train_acc: 0.886\n",
            "valid_loss: 0.304, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.62it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.140, train_acc: 0.953\n",
            "valid_loss: 0.369, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.09it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.069, train_acc: 0.979\n",
            "valid_loss: 0.483, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.31it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.989\n",
            "valid_loss: 0.534, valid_acc: 0.861\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 87.17it/s]\n",
            "test_loss: 0.301, test_acc: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nViqukR1TJiE"
      },
      "source": [
        "### Lab 2 (b): Study of GRU Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q79rYvVFTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff670b93-4103-40d5-d03f-809fc453cf46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 87,560 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.81it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.66it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.513\n",
            "valid_loss: 0.692, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.03it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.605, train_acc: 0.721\n",
            "valid_loss: 0.539, valid_acc: 0.817\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.20it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.473, train_acc: 0.850\n",
            "valid_loss: 0.476, valid_acc: 0.847\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.47it/s]\n",
            "Saving... \n",
            "epoch: 4\n",
            "train_loss: 0.413, train_acc: 0.873\n",
            "valid_loss: 0.446, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.21it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.369, train_acc: 0.888\n",
            "valid_loss: 0.449, valid_acc: 0.858\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 90.94it/s]\n",
            "test_loss: 0.438, test_acc: 0.861\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adagrad'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, \"gru_1layer_adagrad\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, \"gru_1layer_rmsprop\", override_models_with_gru=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwVxSfWuziAS",
        "outputId": "7aab59d0-db49-45aa-dae7-d0eedacc7ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 87,560 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.36it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.505, train_acc: 0.727\n",
            "valid_loss: 0.281, valid_acc: 0.890\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.05it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 91.26it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.189, train_acc: 0.929\n",
            "valid_loss: 0.282, valid_acc: 0.891\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.03it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 90.57it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.091, train_acc: 0.968\n",
            "valid_loss: 0.319, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 53.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.96it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.045, train_acc: 0.985\n",
            "valid_loss: 0.489, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:06<00:00, 52.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.03it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.025, train_acc: 0.991\n",
            "valid_loss: 0.580, valid_acc: 0.867\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.89it/s]\n",
            "test_loss: 0.281, test_acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "_ = train_and_test_model_with_hparams(new_hparams, \"gru_1layer_adam\", override_models_with_gru=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvSCOB0I0lab",
        "outputId": "af129913-1616-4c97-ba76-ae74fb44077f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 87,560 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.83it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.645, train_acc: 0.607\n",
            "valid_loss: 0.341, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.40it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.258, train_acc: 0.897\n",
            "valid_loss: 0.289, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.51it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.15it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.119, train_acc: 0.958\n",
            "valid_loss: 0.315, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 51.65it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.75it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.056, train_acc: 0.982\n",
            "valid_loss: 0.404, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.13it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.027, train_acc: 0.992\n",
            "valid_loss: 0.489, valid_acc: 0.871\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 85.52it/s]\n",
            "test_loss: 0.289, test_acc: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwzNL0BATJiE"
      },
      "source": [
        "### Lab 2 (c) Deeper LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igVKHQ-VTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e1f70d-ada6-4903-cc95-9e972ab6853f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 178,660 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 28.74it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.84it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.584, train_acc: 0.645\n",
            "valid_loss: 0.419, valid_acc: 0.819\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.04it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.261, train_acc: 0.897\n",
            "valid_loss: 0.314, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.80it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.145, train_acc: 0.949\n",
            "valid_loss: 0.532, valid_acc: 0.827\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.35it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 64.85it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.085, train_acc: 0.972\n",
            "valid_loss: 0.396, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.62it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.41it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.049, train_acc: 0.985\n",
            "valid_loss: 0.459, valid_acc: 0.876\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 69.40it/s]\n",
            "test_loss: 0.308, test_acc: 0.880\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.N_LAYERS = 3\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_3layer_adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kptCy4CX1r7i",
        "outputId": "301ec88b-20f4-4b74-8251-1e7e58b88032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 259,460 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 26.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.62it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.666, train_acc: 0.553\n",
            "valid_loss: 0.510, valid_acc: 0.771\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.19it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.43it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.301, train_acc: 0.875\n",
            "valid_loss: 0.286, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.72it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.139, train_acc: 0.951\n",
            "valid_loss: 0.305, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 59.05it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.078, train_acc: 0.975\n",
            "valid_loss: 0.399, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 59.01it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.990\n",
            "valid_loss: 0.538, valid_acc: 0.879\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 60.57it/s]\n",
            "test_loss: 0.286, test_acc: 0.887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.N_LAYERS = 4\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_4layer_adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz5UBSRE11cP",
        "outputId": "3ecd7ba4-189a-46cd-fd06-e363e2d2e500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 340,260 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:19<00:00, 19.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.72it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.58it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.694, train_acc: 0.503\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.34it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.43it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.693, train_acc: 0.501\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 21.91it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.46it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.693, train_acc: 0.503\n",
            "valid_loss: 0.693, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.00it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.72it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.693, train_acc: 0.502\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 49.16it/s]\n",
            "test_loss: 0.693, test_acc: 0.502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUrDm-4CTJiE"
      },
      "source": [
        "### Lab 2 (d) Wider LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv8CW6uZTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328967c1-79f6-4ffb-9b79-3fa8edcfa7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 148,560 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.05it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.640, train_acc: 0.604\n",
            "valid_loss: 0.363, valid_acc: 0.844\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.36it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.51it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.281, train_acc: 0.887\n",
            "valid_loss: 0.288, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.11it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.141, train_acc: 0.950\n",
            "valid_loss: 0.298, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.55it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.48it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.086, train_acc: 0.972\n",
            "valid_loss: 0.376, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.71it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.058, train_acc: 0.981\n",
            "valid_loss: 0.423, valid_acc: 0.865\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 72.21it/s]\n",
            "test_loss: 0.281, test_acc: 0.888\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.HIDDEN_DIM = 150\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_hidden150')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.HIDDEN_DIM = 200\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_hidden200')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyIfGgiQ6Kxr",
        "outputId": "1541da9b-5af1-4b66-9007-97e807e38941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 219,260 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 76.64it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.675, train_acc: 0.584\n",
            "valid_loss: 0.448, valid_acc: 0.822\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.66it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.20it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.307, train_acc: 0.875\n",
            "valid_loss: 0.308, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.18it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 79.79it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.168, train_acc: 0.941\n",
            "valid_loss: 0.315, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 45.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 78.74it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.131, train_acc: 0.958\n",
            "valid_loss: 0.371, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 77.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.061, train_acc: 0.982\n",
            "valid_loss: 0.416, valid_acc: 0.880\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 77.68it/s]\n",
            "test_loss: 0.295, test_acc: 0.883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.HIDDEN_DIM = 250\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_hidden250')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynhlOXwW5E58",
        "outputId": "d01c989f-81ca-4685-f764-fdce59fb538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 309,960 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.26it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.68it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.691, train_acc: 0.557\n",
            "valid_loss: 0.654, valid_acc: 0.616\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.86it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.393, train_acc: 0.833\n",
            "valid_loss: 0.306, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.14it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.47it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.181, train_acc: 0.933\n",
            "valid_loss: 0.295, valid_acc: 0.883\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.06it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.25it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.112, train_acc: 0.963\n",
            "valid_loss: 0.325, valid_acc: 0.868\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.16it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.068, train_acc: 0.980\n",
            "valid_loss: 0.391, valid_acc: 0.872\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.59it/s]\n",
            "test_loss: 0.283, test_acc: 0.887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.HIDDEN_DIM = 300\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_hidden300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCWcDAdl6oDr",
        "outputId": "4a8d1c7b-f7d1-4727-dd2e-d192ea8089b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 420,660 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.04it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.680, train_acc: 0.557\n",
            "valid_loss: 0.646, valid_acc: 0.592\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.28it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.04it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.404, train_acc: 0.818\n",
            "valid_loss: 0.326, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.46it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.188, train_acc: 0.930\n",
            "valid_loss: 0.337, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.12it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.26it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.106, train_acc: 0.965\n",
            "valid_loss: 0.357, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.15it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 46.56it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.062, train_acc: 0.981\n",
            "valid_loss: 0.424, valid_acc: 0.878\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 54.16it/s]\n",
            "test_loss: 0.322, test_acc: 0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akGouTGiTJiE"
      },
      "source": [
        "### Lab 2 (e) Larger Embedding Table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42MkTmjz_bY9",
        "outputId": "2c001185-3310-498d-c68b-078d4018ef3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 154,718 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.47it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.93it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.591, train_acc: 0.630\n",
            "valid_loss: 0.331, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.88it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 80.90it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.229, train_acc: 0.912\n",
            "valid_loss: 0.277, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.41it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.115, train_acc: 0.961\n",
            "valid_loss: 0.348, valid_acc: 0.881\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.75it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.24it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.065, train_acc: 0.979\n",
            "valid_loss: 0.431, valid_acc: 0.884\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.41it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.73it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.034, train_acc: 0.990\n",
            "valid_loss: 0.481, valid_acc: 0.874\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 86.72it/s]\n",
            "test_loss: 0.274, test_acc: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 4\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ddM9Ew_8l5",
        "outputId": "4f0c0080-237f-4cfd-ef04-38220cc06cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 268,434 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.27it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.541, train_acc: 0.684\n",
            "valid_loss: 0.320, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.46it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.33it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.214, train_acc: 0.917\n",
            "valid_loss: 0.295, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.85it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.104, train_acc: 0.966\n",
            "valid_loss: 0.348, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 46.44it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.17it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.086, train_acc: 0.969\n",
            "valid_loss: 0.409, valid_acc: 0.845\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.62it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.25it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.040, train_acc: 0.987\n",
            "valid_loss: 0.539, valid_acc: 0.875\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 64.09it/s]\n",
            "test_loss: 0.291, test_acc: 0.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 8\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU8wsuLX9s-G",
        "outputId": "eae719cc-486d-4c92-c0f7-0edd2e9ee7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 495,866 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.97it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.512, train_acc: 0.706\n",
            "valid_loss: 0.316, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.82it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.202, train_acc: 0.924\n",
            "valid_loss: 0.296, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 89.40it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.099, train_acc: 0.968\n",
            "valid_loss: 0.375, valid_acc: 0.873\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.46it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.52it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.054, train_acc: 0.982\n",
            "valid_loss: 0.469, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.04it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.96it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.041, train_acc: 0.987\n",
            "valid_loss: 0.529, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 86.72it/s]\n",
            "test_loss: 0.296, test_acc: 0.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MQ9OyBDTJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52478eac-0510-4d2c-ba6a-5509d17738a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 2,883,902 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.43it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.75it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.474, train_acc: 0.740\n",
            "valid_loss: 0.313, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.11it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.193, train_acc: 0.931\n",
            "valid_loss: 0.324, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 43.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.79it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.095, train_acc: 0.969\n",
            "valid_loss: 0.389, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.24it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 34.12it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.050, train_acc: 0.984\n",
            "valid_loss: 0.492, valid_acc: 0.861\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 40.11it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 65.56it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.029, train_acc: 0.990\n",
            "valid_loss: 0.593, valid_acc: 0.867\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 65.71it/s]\n",
            "test_loss: 0.303, test_acc: 0.878\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 50\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed50')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 100\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdSSfI-X9Z9_",
        "outputId": "b1749fee-669d-40b7-c3bc-e40d4afc59a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 5,726,802 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 36.03it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.90it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.453, train_acc: 0.768\n",
            "valid_loss: 0.328, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 45.10it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.46it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.197, train_acc: 0.927\n",
            "valid_loss: 0.312, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.72it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 34.24it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.093, train_acc: 0.970\n",
            "valid_loss: 0.381, valid_acc: 0.874\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.75it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.056, train_acc: 0.982\n",
            "valid_loss: 0.499, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 44.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.28it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.038, train_acc: 0.988\n",
            "valid_loss: 0.624, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 82.65it/s]\n",
            "test_loss: 0.315, test_acc: 0.877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 150\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed150')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IU-l62c9f7C",
        "outputId": "f73a654c-dff6-4a06-9157-09b0b9412482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 8,569,702 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.29it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.435, train_acc: 0.784\n",
            "valid_loss: 0.312, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 42.23it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.29it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.214, train_acc: 0.920\n",
            "valid_loss: 0.355, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.75it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.091, train_acc: 0.970\n",
            "valid_loss: 0.398, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.12it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.053, train_acc: 0.983\n",
            "valid_loss: 0.484, valid_acc: 0.859\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:08<00:00, 41.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.02it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.040, train_acc: 0.987\n",
            "valid_loss: 0.593, valid_acc: 0.860\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 84.93it/s]\n",
            "test_loss: 0.310, test_acc: 0.880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 200\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed200')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0hs9VGk9jdC",
        "outputId": "180fe271-ea06-4db3-a906-403abc51aec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 11,412,602 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 38.86it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 53.63it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.460, train_acc: 0.771\n",
            "valid_loss: 0.338, valid_acc: 0.858\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.85it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.90it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.198, train_acc: 0.926\n",
            "valid_loss: 0.299, valid_acc: 0.886\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 37.33it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 88.29it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.082, train_acc: 0.972\n",
            "valid_loss: 0.381, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.28it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.25it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.042, train_acc: 0.986\n",
            "valid_loss: 0.558, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 39.50it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.00it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.042, train_acc: 0.986\n",
            "valid_loss: 0.694, valid_acc: 0.847\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 84.94it/s]\n",
            "test_loss: 0.295, test_acc: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 250\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed250')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD5Gnb3Q9nMY",
        "outputId": "fa61f850-1f1f-4dcc-f6de-ab875ddd65de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 14,255,502 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.94it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.437, train_acc: 0.786\n",
            "valid_loss: 0.302, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.68it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.88it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.187, train_acc: 0.933\n",
            "valid_loss: 0.301, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 81.38it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.081, train_acc: 0.973\n",
            "valid_loss: 0.401, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 36.90it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 85.97it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.040, train_acc: 0.987\n",
            "valid_loss: 0.554, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:09<00:00, 37.25it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.32it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.034, train_acc: 0.989\n",
            "valid_loss: 0.553, valid_acc: 0.858\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 84.71it/s]\n",
            "test_loss: 0.295, test_acc: 0.883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvwg-Ty4TJiE"
      },
      "source": [
        "### Lab 2(f) Compound scaling of embedding_dim, hidden_dim, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlrVnIW0TJiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d971ce-5ab3-4954-c3c4-da5919414cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 568,018 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.27it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 46.14it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.643, train_acc: 0.594\n",
            "valid_loss: 0.478, valid_acc: 0.828\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 58.51it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.329, train_acc: 0.867\n",
            "valid_loss: 0.351, valid_acc: 0.836\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.80it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.169, train_acc: 0.940\n",
            "valid_loss: 0.333, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 24.84it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.32it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.090, train_acc: 0.972\n",
            "valid_loss: 0.415, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:14<00:00, 25.09it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 56.87it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.054, train_acc: 0.984\n",
            "valid_loss: 0.481, valid_acc: 0.869\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 57.90it/s]\n",
            "test_loss: 0.323, test_acc: 0.880\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 150\n",
        "new_hparams.N_LAYERS = 3\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_3layer_adam_embed2_hidden150')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 150\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden150')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1IGKZZZB1rh",
        "outputId": "190c4ee0-8d85-4f28-ad5c-df3951b5a475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 386,818 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 66.38it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.645, train_acc: 0.606\n",
            "valid_loss: 0.652, valid_acc: 0.605\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.97it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.348, train_acc: 0.850\n",
            "valid_loss: 0.378, valid_acc: 0.833\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.77it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.37it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.184, train_acc: 0.932\n",
            "valid_loss: 0.311, valid_acc: 0.877\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.78it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.08it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.087, train_acc: 0.972\n",
            "valid_loss: 0.362, valid_acc: 0.869\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.44it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.62it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.048, train_acc: 0.987\n",
            "valid_loss: 0.467, valid_acc: 0.867\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.54it/s]\n",
            "test_loss: 0.299, test_acc: 0.883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKG7IwiWCiew",
        "outputId": "c39dd3da-45ec-41e0-ec07-d54b7ea1d524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 235,518 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.57it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.77it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.538, train_acc: 0.678\n",
            "valid_loss: 0.321, valid_acc: 0.863\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.03it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.19it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.223, train_acc: 0.917\n",
            "valid_loss: 0.286, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.06it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.107, train_acc: 0.965\n",
            "valid_loss: 0.370, valid_acc: 0.882\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.16it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.97it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.058, train_acc: 0.982\n",
            "valid_loss: 0.497, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.53it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.040, train_acc: 0.988\n",
            "valid_loss: 0.542, valid_acc: 0.875\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 72.83it/s]\n",
            "test_loss: 0.279, test_acc: 0.889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 120\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden120')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syeIcVitFEw7",
        "outputId": "d40352b7-bf8a-4490-8831-225f0e4f13bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 288,838 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.78it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.55it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.622, train_acc: 0.624\n",
            "valid_loss: 0.481, valid_acc: 0.778\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.78it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.299, train_acc: 0.878\n",
            "valid_loss: 0.325, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.63it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.155, train_acc: 0.945\n",
            "valid_loss: 0.358, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.01it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.68it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.077, train_acc: 0.975\n",
            "valid_loss: 0.380, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.94it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.67it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.989\n",
            "valid_loss: 0.586, valid_acc: 0.862\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.29it/s]\n",
            "test_loss: 0.325, test_acc: 0.869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 3\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 1\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed3_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4uNHYoqFc6s",
        "outputId": "f3b3860d-1046-4750-9d33-cf8ccff9f505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 211,576 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 49.07it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 87.95it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.560, train_acc: 0.662\n",
            "valid_loss: 0.330, valid_acc: 0.862\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 48.37it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 83.91it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.221, train_acc: 0.915\n",
            "valid_loss: 0.305, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 47.99it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 84.96it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.112, train_acc: 0.962\n",
            "valid_loss: 0.331, valid_acc: 0.871\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.36it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.96it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.064, train_acc: 0.980\n",
            "valid_loss: 0.402, valid_acc: 0.878\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:07<00:00, 50.19it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 86.98it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.988\n",
            "valid_loss: 0.506, valid_acc: 0.869\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 89.39it/s]\n",
            "test_loss: 0.296, test_acc: 0.884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 3\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed3_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2_Cy_sEGHrU",
        "outputId": "80816f83-d369-4e4b-f94f-120409b0aaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 292,376 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:12<00:00, 29.88it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.01it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.604, train_acc: 0.644\n",
            "valid_loss: 0.333, valid_acc: 0.866\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.16it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.257, train_acc: 0.899\n",
            "valid_loss: 0.300, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.20it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.77it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.128, train_acc: 0.956\n",
            "valid_loss: 0.334, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.48it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.25it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.066, train_acc: 0.979\n",
            "valid_loss: 0.421, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.82it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.48it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.990\n",
            "valid_loss: 0.545, valid_acc: 0.863\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.43it/s]\n",
            "test_loss: 0.303, test_acc: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 3\n",
        "new_hparams.HIDDEN_DIM = 150\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_1layer_adam_embed3_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hw9m5F1GOur",
        "outputId": "12724f4d-f7d5-47ec-bcf3-c4854c83fc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 443,876 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.59it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.99it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.575, train_acc: 0.671\n",
            "valid_loss: 0.321, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 33.00it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.16it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.241, train_acc: 0.908\n",
            "valid_loss: 0.307, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.71it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.41it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.124, train_acc: 0.958\n",
            "valid_loss: 0.344, valid_acc: 0.864\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.83it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.14it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.071, train_acc: 0.977\n",
            "valid_loss: 0.398, valid_acc: 0.841\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.98it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 67.53it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.044, train_acc: 0.986\n",
            "valid_loss: 0.562, valid_acc: 0.873\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.94it/s]\n",
            "test_loss: 0.302, test_acc: 0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 1\n",
        "new_hparams.HIDDEN_DIM = 150\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed1_hidden150')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzbP97JEERK8",
        "outputId": "9028868a-a843-4c9a-ca04-00e699ae7652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 329,760 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.86it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.20it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.629, train_acc: 0.612\n",
            "valid_loss: 0.493, valid_acc: 0.809\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.04it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.285, train_acc: 0.885\n",
            "valid_loss: 0.314, valid_acc: 0.865\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.89it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.148, train_acc: 0.947\n",
            "valid_loss: 0.331, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.34it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 68.72it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.073, train_acc: 0.978\n",
            "valid_loss: 0.418, valid_acc: 0.854\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 33.54it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 69.82it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.040, train_acc: 0.988\n",
            "valid_loss: 0.474, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 71.10it/s]\n",
            "test_loss: 0.312, test_acc: 0.870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 3\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_3layer_adam_embed2_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7RmC0t9DTbi",
        "outputId": "ab93c58a-f2de-4839-df72-248b0da54f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 316,318 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.49it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 59.72it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.559, train_acc: 0.668\n",
            "valid_loss: 0.359, valid_acc: 0.841\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.71it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 59.20it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.237, train_acc: 0.909\n",
            "valid_loss: 0.281, valid_acc: 0.885\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 60.72it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.127, train_acc: 0.957\n",
            "valid_loss: 0.360, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.56it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.35it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.064, train_acc: 0.981\n",
            "valid_loss: 0.422, valid_acc: 0.845\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:13<00:00, 27.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 61.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.132, train_acc: 0.949\n",
            "valid_loss: 0.420, valid_acc: 0.864\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 62.38it/s]\n",
            "test_loss: 0.279, test_acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_rmsprop_embed2_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEPJBSQOC-YE",
        "outputId": "c7578206-094c-4d37-ffd5-59468210287e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 235,518 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.60it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 72.59it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.686, train_acc: 0.526\n",
            "valid_loss: 0.692, valid_acc: 0.498\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 32.97it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 46.72it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.526, train_acc: 0.719\n",
            "valid_loss: 0.391, valid_acc: 0.831\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 33.02it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.45it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.274, train_acc: 0.890\n",
            "valid_loss: 0.311, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 35.69it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.54it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.177, train_acc: 0.936\n",
            "valid_loss: 0.345, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 31.89it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.48it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.113, train_acc: 0.963\n",
            "valid_loss: 0.397, valid_acc: 0.869\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 71.74it/s]\n",
            "test_loss: 0.308, test_acc: 0.871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "new_hparams.DROPOUT_RATE = 0.5\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzuKs6uueczi",
        "outputId": "933bbf97-9feb-4ffb-b8e0-e84bdbf10285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 235,518 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:11<00:00, 30.47it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.54it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.550, train_acc: 0.713\n",
            "valid_loss: 0.409, valid_acc: 0.828\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.62it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 70.64it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.321, train_acc: 0.873\n",
            "valid_loss: 0.307, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.53it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 73.01it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.167, train_acc: 0.940\n",
            "valid_loss: 0.324, valid_acc: 0.887\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.70it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.19it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.098, train_acc: 0.966\n",
            "valid_loss: 0.347, valid_acc: 0.870\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:10<00:00, 34.63it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<00:00, 71.24it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.057, train_acc: 0.982\n",
            "valid_loss: 0.423, valid_acc: 0.866\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:01<00:00, 70.22it/s]\n",
            "test_loss: 0.297, test_acc: 0.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlILBl-eTJiE"
      },
      "source": [
        "### Lab 2 (g) Bi-Directional LSTM, using best architecture from (f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "towWpxXVTJiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1ea3c5-00c2-4b5a-dca0-0b501e6783c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 437,918 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 21.76it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.58it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.571, train_acc: 0.669\n",
            "valid_loss: 0.580, valid_acc: 0.710\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:18<00:00, 20.21it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.18it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.412, train_acc: 0.832\n",
            "valid_loss: 0.392, valid_acc: 0.840\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 21.93it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.71it/s]\n",
            "Saving... \n",
            "epoch: 3\n",
            "train_loss: 0.233, train_acc: 0.914\n",
            "valid_loss: 0.326, valid_acc: 0.867\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.29it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 47.74it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.138, train_acc: 0.955\n",
            "valid_loss: 0.372, valid_acc: 0.872\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:17<00:00, 20.86it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 47.25it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.079, train_acc: 0.977\n",
            "valid_loss: 0.465, valid_acc: 0.836\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 41.58it/s]\n",
            "test_loss: 0.321, test_acc: 0.867\n"
          ]
        }
      ],
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'adam'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "new_hparams.BIDIRECTIONAL = True\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden100')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "new_hparams.BIDIRECTIONAL = True\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDcDqnM7HkDt",
        "outputId": "a2ab9bbe-f926-4d96-87cf-80c921ab5655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 437,918 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.22it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.06it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.621, train_acc: 0.647\n",
            "valid_loss: 0.356, valid_acc: 0.855\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.52it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.39it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.261, train_acc: 0.895\n",
            "valid_loss: 0.272, valid_acc: 0.888\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.73it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.70it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.131, train_acc: 0.955\n",
            "valid_loss: 0.360, valid_acc: 0.860\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.42it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 50.08it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.074, train_acc: 0.975\n",
            "valid_loss: 0.387, valid_acc: 0.876\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:16<00:00, 22.65it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.96it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.047, train_acc: 0.985\n",
            "valid_loss: 0.429, valid_acc: 0.874\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 48.66it/s]\n",
            "test_loss: 0.268, test_acc: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_hparams = HyperParams()\n",
        "new_hparams.OPTIM = 'rmsprop'\n",
        "new_hparams.LR = 0.001\n",
        "new_hparams.EMBEDDING_DIM = 2\n",
        "new_hparams.HIDDEN_DIM = 100\n",
        "new_hparams.N_LAYERS = 2\n",
        "new_hparams.BIDIRECTIONAL = True\n",
        "_ = train_and_test_model_with_hparams(new_hparams, 'lstm_2layer_adam_embed2_hidden100', override_models_with_gru=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndlFIQBAHwho",
        "outputId": "bbfa5b44-8cbf-46f3-92c4-76c138a4f2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 56458\n",
            "The model has 356,718 trainable parameters\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.17it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 48.03it/s]\n",
            "Saving... \n",
            "epoch: 1\n",
            "train_loss: 0.696, train_acc: 0.630\n",
            "valid_loss: 0.508, valid_acc: 0.752\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:19<00:00, 19.20it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.43it/s]\n",
            "Saving... \n",
            "epoch: 2\n",
            "train_loss: 0.291, train_acc: 0.879\n",
            "valid_loss: 0.295, valid_acc: 0.879\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.36it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.36it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.139, train_acc: 0.950\n",
            "valid_loss: 0.306, valid_acc: 0.880\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.71it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 51.84it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.975\n",
            "valid_loss: 0.385, valid_acc: 0.875\n",
            "training...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365/365 [00:15<00:00, 23.32it/s]\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:01<00:00, 49.11it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.988\n",
            "valid_loss: 0.466, valid_acc: 0.871\n",
            "evaluating...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:02<00:00, 48.91it/s]\n",
            "test_loss: 0.287, test_acc: 0.880\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}